        <section id="publications">
            <h2>Research</h2>
            <div class="publications-grid">
                <article class="publication-card">
                    <div class="publication-content">
                        <h3 class="paper-title">CL-Splats: Continual Learning of Gaussian Splatting with Local Optimization</h3>
                        <p class="authors"><b>Jan Ackermann</b>, Jonas Kulhanek, Shengqu Cai, Haofei Xu, Marc Pollefeys, Gordon
                            Wetzstein, Leonidas Guibas, Songyou Peng</p>
                        <p class="conference">International Conference on Computer Vision (ICCV), <span class="year">2025</span></p>
                        <div class="paper-description">
                            <p class="tldr"><span class="tldr-label">TL;DR:</span> We explore how to adapt an existing 3DGS scene representation to new inceremental
                                changes. We propose a novel and efficient way to identify changed regions and then to locally
                                optimize them. This not only produces more accurate scene updates but also enables new applications.
                            </p>
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2506.21117" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-pdf" aria-hidden="true"></i> Arxiv</a>
                            <a href="https://cl-splats.github.io/" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-globe" aria-hidden="true"></i> Website</a>
                        </div>
                    </div>
                </article>

                <article class="publication-card">
                    <div class="publication-content">
                        <h3 class="paper-title">AIpparel: A Multimodal Foundation Model for Digital Garments</h3>
                        <p class="authors">Kiyohiro Nakayama*, <b>Jan Ackermann*</b>, Timur L. Kesdogan*, Yang Zheng, Maria
                            Korosteleva, Olga Sorkine-Hornung, Leonidas Guibas, Guandao Yang, Gordon Wetzstein</p>
                        <p class="conference">Computer Vision and Pattern Recognition (CVPR), <span class="year">2025</span> <span class="badge">Highlight</span</p>
                        <div class="paper-description">
                            <p class="tldr"><span class="tldr-label">TL;DR:</span> We introduce AIpparel, the first large-scale multimodal generative model designed
                                specifically for digital garments. By extending LLaVA to incorporate a new garment modality,
                                AIpparel enables the creation of sewing patterns from text, image, and garment inputs.</p>
                        </div>
                        <div class="paper-links">
                            <a href="https://cvpr.thecvf.com/virtual/2025/poster/35255" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-pdf" aria-hidden="true"></i> Paper</a>
                            <a href="https://arxiv.org/abs/2412.03937" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-pdf" aria-hidden="true"></i> Arxiv</a>
                            <a href="https://georgenakayama.github.io/AIpparel/" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-globe" aria-hidden="true"></i> Website</a>
                        </div>
                    </div>
                </article>

                <article class="publication-card">
                    <div class="publication-content">
                        <h3 class="paper-title">Do Efficient Transformers Really Save Computation?</h3>
                        <p class="authors">Kai Yang, <b>Jan Ackermann</b>, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng,
                            Qiwei Ye, Di He, Liwei Wang</p>
                        <p class="conference">International Conference on Machine Learning (ICML), <span class="year">2024</span></p>
                        <div class="paper-description">
                            <p class="tldr"><span class="tldr-label">TL;DR:</span> We explore the class of Linear and Sparse Transformers in a Chain-of-Thought
                                (CoT) setting, finding that to match the performance of regular Transformers, their hidden
                                dimensions must scale with the problem size.</p>
                        </div>
                        <div class="paper-links">
                            <a href="https://icml.cc/virtual/2024/poster/32716" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-pdf" aria-hidden="true"></i> Paper</a>
                            <a href="https://arxiv.org/abs/2402.13934" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-alt" aria-hidden="true"></i> Arxiv</a>
                        </div>
                    </div>
                </article>

                <article class="publication-card">
                    <div class="publication-content">
                        <h3 class="paper-title">Maskomaly: Zero-shot Mask Anomaly Segmentation</h3>
                        <p class="authors"><b>Jan Ackermann</b>, Christos Sakaridis, Fisher Yu</p>
                        <p class="conference">British Machine Vision Conference (BMVC), <span class="year">2023</span> <span class="badge">Oral</span></p>
                        <div class="paper-description">
                            <p class="tldr"><span class="tldr-label">TL;DR:</span> We show that pretrained Mask-based segmentation models can predict anomalies
                                without further tuning. Additionally, we introduce a metric for anomaly segmentation that favors
                                models with confident predictions.</p>
                        </div>
                        <div class="paper-links">
                            <a href="https://proceedings.bmvc2023.org/329/" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-pdf" aria-hidden="true"></i> Paper</a>
                            <a href="https://arxiv.org/abs/2305.16972" target="_blank" rel="noopener" class="paper-link"><i class="fas fa-file-alt" aria-hidden="true"></i> Arxiv</a>
                        </div>
                    </div>
                </article>
            </div>
        </section>

